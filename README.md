Assesment 


This project implements an Intelligent Query Router that dynamically routes natural language queries to the most relevant data sourceâ€”SQL or RAG (vector store)â€”based on intelligent confidence scoring. If both paths fail, the system rephrases and retries the query using an LLM.

ğŸš€ Project Overview 
SQL Agent Tool: LangGraph + LangChain SQL agent wrapped as a tool, executes validated SQL queries over PostgreSQL.

RAG Agent Tool: Embedding-based document retriever (ChromaDB) wrapped as a tool, handles unstructured data (PDFs).


Router Agents:

LangChain ReAct Router: A standard ReAct agent that decides between tools (SQL or RAG) using tool-calling logic.

LangGraph Router: A graph-based pipeline that:

Calls both SQL and RAG tools

Scores both responses using heuristics + cross-encoder

Picks the better result based on confidence

Optionally rephrases and retries the query if needed

Tool Server (MCP): Hosts both SQL and RAG agents as callable MCP tools. 



ğŸ“ Repository Structure
text
Copy
Edit
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ rag_agent.py           # RAG retrieval agent + tool
â”‚   â”œâ”€â”€ prebuilt_sql_agent.py  # LangGraph SQL graph + tool
â”‚   
â”‚
â”œâ”€â”€ MCP/
â”‚   â”œâ”€â”€ tools.py               # Tool loader/registrar (RAG + SQL)
â”‚   â””â”€â”€ mcp_server.py          # Launches MCP tool server
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ Pdf_embedding.py          # Embeds PDFs to ChromaDB
â”‚   â”œâ”€â”€ setup_postgres.py         # Loads CSVs into PostgreSQL
â”‚   
â”‚
â”œâ”€â”€ guardrails/
â”‚   â”œâ”€â”€ sql_guardrails.py      # SQL row-level confidence scoring
â”‚   â””â”€â”€ rag_guardrails.py      # Answer string-level scoring
â”‚
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ sql_prompts.py         # Prompts for SQL generation/checking
â”‚   â””â”€â”€ rag_prompts.py         # Prompts for RAG retrieval
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_sql.py            # Unit tests for SQL agent   ## DONT USE
â”‚   â”œâ”€â”€ test_rag.py            # Unit tests for RAG agent
â”‚   â””â”€â”€ test_router.py         # Full router tests
â”‚
â”œâ”€â”€ gradio_ui/
â”‚   â””â”€â”€ app.py                 # Gradio UI interface for user input
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ example_run.py         # Optional runners / experiments
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ csv_data.csv           # Structured tabular example
â”‚   â””â”€â”€ pdf_docs/              # Sample PDFs for RAG
â”‚
â”œâ”€â”€ chroma_db/                 # Local ChromaDB storage
â”œâ”€â”€ .env                       # API keys and DB URL config
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # You are here
ğŸ› ï¸ Setup
Create & activate environment (Python 3.11)

Conda:

bash
Copy
Edit
conda create -n "env Name" python=3.11 -y
conda activate venv
venv:

bash
Copy
Edit
python3.11 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
Install dependencies:

bash
Copy
Edit
pip install -r requirements.txt
Install optional guard-rail models (for confidence scoring):

bash
Copy
Edit
pip install sentence-transformers torch numpy
Configure .env:

Create a .env file with:

dotenv
Copy
Edit
DATABASE_URL=postgresql://user:pass@localhost:5432/your_db
OPENAI_API_KEY=sk-...
CE_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2   


âš™ï¸ Running the System
Step 1: Host the MCP Tool Server
This serves the SQL and RAG agents as callable tools.

bash
Copy
Edit
python -m MCP.mcp_server
Keep it running in one terminal.

Step 2: Run the Query Router
In a new terminal, run the router interface:

bash
Copy
Edit
python -m agents.router
# â“  Enter your question: Who submitted the compliance report?
Output will show the selected tool, answer, and confidence score.

Optional: Run Gradio UI
bash
Copy
Edit
python gradio_ui/app.py
This launches a browser-based interface to test the system interactively.

ğŸ” Confidence Scoring
1. SQL Confidence (compute_sql_confidence)
Row volume: min(#rows / 10, 1.0)

Null coverage: 1 âˆ’ (null_cells / total_cells)

Cross-encoder score for top-6 rows

Weighted sum: 0.3 Ã— volume + 0.2 Ã— coverage + 0.5 Ã— encoder

2. Answer Confidence (compute_answer_confidence)
Cross-encoder similarity between question and answer

Penalizes overly short or lengthy answers

Checks if numeric tokens match

Final Confidence
text
Copy
Edit
confidence = 0.7 Ã— sql_score + 0.3 Ã— answer_score
ğŸ“– Architecture Overview
ğŸ§  SQL Agent
LangGraph-based

Nodes: list_tables â†’ get_schema â†’ generate â†’ check â†’ run

Exposed via MCP as a tool

ğŸ“„ RAG Agent
Embeds PDF/CSV to ChromaDB

Retrieves top-k docs

Synthesizes answer via LLM

Exposed via MCP as a tool

ğŸ”€ Query Router
Step 1: Try SQL Agent â†’ compute confidence

Step 2: Fallback to RAG â†’ compare score

Step 3: Rephrase and retry if both are low

ğŸ§ª Running Tests
bash
Copy
Edit
pytest tests/
Make sure SQL and Chroma data are preloaded.

ğŸ“œ License
This project is licensed under the MIT License. See LICENSE for details.